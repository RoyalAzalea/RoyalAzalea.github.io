---
layout: post
title: "Tensorflow Study _ LEC 02 : Linear Regression"
date: 2017-08-28
categories: [tensorflow, machine_learning]
---

`Linear Regression`은 `Supervised Learning`의 한 종류로, 학습을 통해 `Traning Data`로
 부터 선형모델을 만든다.


 **"예를 들어, 학생들의 '공부시간 - 시험점수' 관한 Data가 있다. 이때, 공부시간이
 많은 학생일수록 시험점수가 좋다고 가정해보자. 이 Data를 학습시킨 후, 시험보기 전
 내가 공부한 시간을 모델에 입력하면 대략 몇 점이 나올 것이라고 답이 나온다. 그리고 그
 답으로 나온 점수는 공부한 시간이 많다면 높게, 공부한 시간이 적다면 낮게 나온다.
 왜냐하면, 공부시간이 많을 수록 시험점수가 좋은 Data로 학습을 시켰기 때문이다."**


이처럼, `Linear Regression`을 통해 가설(`Hypothesis`)을 세울 수 있다. 이 가설은
우리가 `Traning Data`로 부터 학습한 모델이 선형(`Linear`)일 것이라고 예측한다. 즉,
어떤 Data들을 학습해서 여기에 잘 맞는 **선형모델 or 선** 을 찾는 것 이다. 참고로
학습을 시킬 때, $x$는 예측을 하기위한 기본적인 자료로 `Feature`라고 불리며, $y$는
해당 $x$의 결과이다.


이제 `Linear Regression`을 통해 선을 찾는것을 알게됐다. 근데, 우리가 찾은 선들 중
어떤 선이 좋은 선일까? 우리가 세운 가설과 실제 값이 얼마나 다른지 측정하면 된다.
**즉, 우리가 측정한 가설과 실제 점들 사이의 거리가 가까우면 좋은 선이고, 멀면 좋지
않은 선이다. 그리고 우리가 세운 가설과 실제 값이 얼마나 다른지 확인하는 함수를
비용함수(cost function or loss function)라고 한다.**  


그림


이제 `cost function`을 이용해서 최적의 선을 찾는 방법을 보겠다.
우선, `Linear Regression`에서 `Hypothesis`를 1차 방정식이라고 생각해보자.
그럼 다음과 같은 식이 나온다.


$$
H(x) = Wx + b \label{1}\tag{1}
$$


그리고 실제 값과 예측 값의 차이는 선과 점들 사이의 거리를 계산해서 구하면 된다.
예측 값과 실제 값과의 차이는 $H(x) - y$로 구할 수 있다. 하지만, 이 식은 음수가
나오므로 좋지않다. 앞의 식 대신 $(H(x) - y)^2$을 사용한다. 이 식은 제곱을 하므로
나오는 값이 모두 양수가 되고, 예측 값과 실제 값의 차이가 크면 `cost`가 커지므로
패널티도 줄 수 있다.


그럼 이제 위의 식을 기반으로 일반화한 다음과 같은 `cost function`들을 구할 수 있다.


$$
cost_1 = {1 \over m}\sum_{i=i}^m (H(x^(i)) - y^(i))^2 \label{2}\tag{2}
$$

$$
cost_2 = {1 \over 2m}\sum_{i=i}^m (H(x^(i)) - y^(i))^2 \label{3}\tag{3}
$$


여기서 $H(x^{(i)}$는 가정으로 얻은 예측값이고, $y^{(i)}$는 실제 값이다. 그리고
식 $\ref{1}$에서 $H(x)$는 $W$와 $b$의 함수이므로, 식 $\ref{2}$와 $\ref{3}$의
$cost_1$과 $cost_2$도 $W$와 $b$의 함수가 된다.


`cost`가 작을수록 예측 값과 실제 값의 차이가 적으므로, 학습 시 `cost` 값을
최소로 만드는 $W$와 $b$를 구하는게 학습의 목표이다!!
